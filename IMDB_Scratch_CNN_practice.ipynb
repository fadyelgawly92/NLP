{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0FqUuaPnkyo",
        "outputId": "23714692-5310-44e4-afcb-71d4090ce500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import re\n",
        "import html\n",
        "import string\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_PATH=Path('./dat/')\n",
        "DATA_PATH.mkdir(exist_ok=True)\n",
        "#if not os.path.exists('./dat/aclImdb_v1.tar.gz'):\n",
        "if not os.path.exists('./dat/aclImdb'):\n",
        "    !curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "    !tar -xf aclImdb_v1.tar.gz -C {DATA_PATH}"
      ],
      "metadata": {
        "id": "jbDttpl0rKq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "CLASSES = ['neg', 'pos']#, 'unsup']\n",
        "PATH=Path('./dat/aclImdb/')\n",
        "\n",
        "def get_texts(path):\n",
        "    texts,labels = [],[]\n",
        "    for idx,label in enumerate(CLASSES):\n",
        "        for fname in (path/label).glob('*.*'):\n",
        "            #texts.append(fixup(fname.open('r', encoding='utf-8').read()))\n",
        "            texts.append(fname.open('r', encoding='utf-8').read())\n",
        "            labels.append(idx)\n",
        "    #return np.array(texts),np.array(labels)\n",
        "    return texts, labels\n",
        "\n",
        "trn_texts,trn_labels = get_texts(PATH/'train')\n",
        "tst_texts,tst_labels = get_texts(PATH/'test')"
      ],
      "metadata": {
        "id": "hZKjNBvZrfRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in trn_texts[:10]:\n",
        "  print(t)\n",
        "  #print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUAOnR_arhc7",
        "outputId": "a6b9f796-16de-4220-cce9-10fff8f4ce62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What we have here is a film about how the pursuit of money & revenge can corrupt your soul... or something like that. Guy Ritchie, a director known for his reworking of the gangster genre, bites off more than he can chew with this one.<br /><br />His use of modern film noir to tackle the theme of a man setting himself free by swallowing his pride, being nice to his enemy & giving away all his money falls flat on it's face. When Jason Statham's character no longer fears Ray Liotta, it apparently drives Liotta crazy enough to blow his head off in the final scene. Why? Basically you cannot set up a mafiosi like the Liotta character, who has presumably got to his station in life by displaying the kind of ruthless behaviour evident throughout the film, only then to have him driven to suicide by nothing more than a pitying smile on the face of Statham's character.<br /><br />Before anyone starts to say I'm missing the point... I'm not. I get it OK? Opt out of the quest for riches & you'll find true happiness and inner peace. Be nice to your enemy and this will confuse him into self-destruction. This seems to be the gist of the movie and in itself this is not a bad premise for a story, although hardly original. The problem is that Ritchie simply doesn't have the skill as a movie maker to carry it off. At the moment when even Guy Ritchie realises this, he appears to get bored with the story and begins to insert red-herrings: The scene when Statham gets knocked over by a car - Why? The shooting of some scenes as Marvel comic animations... again, why?<br /><br />There are so many loose threads & unanswered questions left at the end of the movie you could get all 2001-ish about it and try figuring them out, or simply accept that there are no answers & each viewer will interpret things in their own way. Myself? I was so bored with the pompous tone of the film that I simply didn't care. Frankly the ending couldn't come too soon so that I didn't have to sit through any more of this pretentious psychobabble.<br /><br />A waste of two hours of my life.\n",
            "The only reason I even gave it a 1 out of 10 is because the option to give it zero out of 10 was not allowed. This was the biggest waste of time I've ever endured. For roughly 75 minutes, you are subjected to the WORST acting (and I don't mean that in a good way either, like as in KILLER NERD which had great horrible acting) and a plot that is not only ridiculous but also has absolutely NOTHING to do with a massacre. The reason I even rented this piece of crap was because it has massacre in the title. That said, there was only one killing in the entire movie and it was pretty lame at that. You spend more time watching the kids bickering and doing yard work than anything. Speaking of the kids, the little boy actor is probably the most irritating child actor since bob from house by the cemetery. Did I mention it was shot on video as well? If you want to throw away money and over an hour of your life, then by all means watch it. But if you savor your hard earned dollars and time, then stray as far away as possible.\n",
            "AntiTrust could have been a great vehicle for Rachael Leigh Cook, but the director cut out her best scenes. In the scenes that she are in, she is just a zombie. She is involved in a sub-plot that is simular to a sub-plot in \"Get Carter\", but she handles the sub-plot better in \"Get Carter\".(I blame the director) The director's homage to Hitchcock was corny. (It's the scene were Ryan Philippe's charactor realizes he may not be able to trust Tim Robbin's charactor, at least I think it's a homage to Hitchcock. The DVD shows the scenes that were cut out. I think the director should have trust his instincts and not listen to the test audiences.\n",
            "The premise is ridiculous, the characters unbelievable, the dialogue trite, and the ending absurd. <br /><br />Believe me, I'm a fan of Kevin Kline, but watching him do a Pepe Le Pew accent for 2 hours as a supposed Frenchman is not nearly as amusing as it sounds.<br /><br />For her part, Meg Ryan is once again as perky and adorable as a (take your pick): kewpie doll, baby, puppy, kitten, whatever you happen to think is the cutest creature on earth. She also bears not the slightest resemblance to a real human being.<br /><br />This movie strikes me as an opportunity seized by buddies Lawrence Kasdan and Kline to vacation in Paris and the south of France while being well-paid for it. So I can't really blame them.\n",
            "How has this piece of crap stayed on TV this long? It's terrible. It makes me want to shoot someone. It's so fake that it is actually worse than a 1940s sci-fi movie. I'd rather have a stroke than watch this nonsense. I remember watching it when it first came out. I thought, hey this could be interesting, then I found out how absolutely, insanely, ridiculously stupid it really was. It was so bad that I actually took out my pocket knife and stuck my hand to the table.<br /><br />Please people, stop watching this and all other reality shows, they're the trash that is jamming the networks and canceling quality programming that requires some thought to create.\n",
            "For me this is a story that starts with some funny jokes regarding Franks fanatasies when he is travelling with a staircase and when he is sitting in business meetings... The problem is that when you have been watching this movie for an hour you will see the same fantasies/funny situations again and again and again. It is to predictable. It is more done as a TV story where you can go away and come back without missing anything.<br /><br />I like Felix Herngren as Frank but that is not enough even when it is a comedy it has to have more variations and some kind of message to it's audience....<br /><br />\n",
            "Elvis Presley plays a \"half-breed\" Native American (\"Indian\") who has to defend his reservation from nasty business tycoons. Everyone likes to get drunk, fight, and make children. Fighting, wrestling, and \"punching out\" each other replace the stereotypical hand-raised expression \"How\"?<br /><br />Although he does have make-up on, it's obvious Elvis is healthier than he appeared in prior films; possibly, he was getting ready for his famous \"comeback\". It couldn't have been because this movie's script was anything to get excited about. Joan Blondell trying to seduce Elvis, and Burgess Meredith in \"war paint\", should be ashamed.<br /><br />The best song is \"Stay Away\" (actually, \"Green Sleeves\" with different lyrics). The most embarrassing song is Elvis' love song to the bull \"Dominic\". There are some surreal scenes, but it never becomes trippy enough to succeed in that genre; though, \"Stay Away, Joe\" might provide some laughs if you're in the right \"mood\".<br /><br />Otherwise, stay away. <br /><br />** Stay Away, Joe (1968) Peter Tewksbury ~ Elvis Presley, Burgess Meredith, Joan Blondell\n",
            "To borrow from Dorothy Parker: This is not a film to be tossed aside<br /><br />lightly. It should be thrown with great force.<br /><br />This is an excruciating mess. And I'm a Greenaway fan.<br /><br />MIND-NUMBINGLY AWFUL<br /><br />\"The Mummy Returns\" has much more artistic merit\n",
            "Well, I have to say, this movie was so bad that I would have walked out if i didn't have to review it for work. ANd the worst part is, I wanted to see it so badly that I drove all over the city, paid $10 parking two times because the newspaper listings were wrong. Vince Vaughn plays the guy he always does -- the only time I've seen him play someone else was in that movie with John Travolta. Anyways, the plot has potential -- it sounded great in the preview, but it is filled with totally ridiculous, predictable, weak plot turn points. And I was hoping that this would be one Christmas movie where Christmas DIDN\"t have to be saved, and that Santa didn't need a replacdmetn, but nope. The only cool part was the sleigh rides, and the little bladck kid was the best character. I'm sure this movie would be great for young kids, but for adults it's so lame that it's chore to sit through.\n",
            "I was dying to see this once I saw the ridiculous MEATBALLS poster and divined that it had to be the best satire ever. What a brilliant idea for a satire--the genre is rife. Unfortunately, the finished product (as I think all involved probably realize) is a catalogue of missed opportunities, not-quite-there performances and (thankfully!) a few extremely hilarious, inspired bits. Janeane Garafolo, who is very striking, looks really bad here, probably because she is uncomfortably struggling to make her flat role funny. David Hyde Pierce is just sad to watch, trying to hard to be funny and looking like a Castro nerd. Molly Shannon is so funny just SEEING her makes you laugh, but somehow her segment fails to snowball into something hysterical. Paul Rudd had great teen mannerisms and was sexy as hell, the other guys are also really funny (the nerdier ones). I think the problem is the director just doesn't move things along at the right pace. He starts out very deadpan, and that sets the monotone. But when he lets things get really outrageous (the drug sequence is the second funniest moment I've had all year in movies, the first also coming in a lame movie: Andrea Martin in ALL OVER THE GUY complaining about the movie IN & OUT), it's just plain funny. I wanted this movie to work so badly, but it just didn't. The clothes and styling for 1981 are 99.9% PERFECT, and the very few songs used are also perfect. This ends up as a medium-bad MAD TV episode, complete with frustratingly overlong sketches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_chars(text):\n",
        "    re1 = re.compile(r'  +')\n",
        "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
        "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x1))\n",
        "\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "def replace_numbers(text):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "\n",
        "def remove_whitespaces(text):\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def remove_stopwords(words, stop_words):\n",
        "    \"\"\"\n",
        "    :param words:\n",
        "    :type words:\n",
        "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "    or\n",
        "    from spacy.lang.en.stop_words import STOP_WORDS\n",
        "    :type stop_words:\n",
        "    :return:\n",
        "    :rtype:\n",
        "    \"\"\"\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in text\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    \"\"\"Lemmatize words in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
        "\n",
        "def text2words(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def normalize_text( text):\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = replace_numbers(text)\n",
        "    words = text2words(text)\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    #words = stem_words(words)# Either stem or lemmatize\n",
        "    words = lemmatize_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "\n",
        "    return ''.join(words)"
      ],
      "metadata": {
        "id": "7yUuekHRrjK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_corpus(corpus):\n",
        "  return [normalize_text(t) for t in corpus]\n",
        "  "
      ],
      "metadata": {
        "id": "xpNu5JG-rm-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trn_texts = normalize_corpus(trn_texts)\n",
        "tst_texts = normalize_corpus(tst_texts)"
      ],
      "metadata": {
        "id": "OUYMuJWRrqN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sz = 10000\n",
        "tok = Tokenizer(num_words=vocab_sz, oov_token='UNK')\n",
        "texts = trn_texts + tst_texts\n",
        "tok.fit_on_texts(texts)"
      ],
      "metadata": {
        "id": "DbsbTeuqrrb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tok.texts_to_sequences(trn_texts)\n",
        "x_test = tok.texts_to_sequences(tst_texts)\n",
        "y_train = np.asarray(trn_labels).astype('float32')\n",
        "y_test = np.asarray(tst_labels).astype('float32')"
      ],
      "metadata": {
        "id": "uTCEjUqrsxJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_len(texts):\n",
        "  return max([len(word_tokenize(text)) for text in texts])\n",
        "\n",
        "def pad_seq(seq, maxlen):  \n",
        "  return np.array(pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post'))"
      ],
      "metadata": {
        "id": "BzG451MOsy0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#maxlen = get_max_len(texts)\n",
        "maxlen = 100\n",
        "x_train = pad_seq(x_train, maxlen)\n",
        "x_test = pad_seq(x_test, maxlen)"
      ],
      "metadata": {
        "id": "k4dQizTXs0ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjfePVVds2Em",
        "outputId": "ba93f370-9e91-476c-ea85-76da6ba81ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 100)\n",
            "(25000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.4, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "EnDR6IIfs5BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, Conv1D, MaxPooling1D\n",
        "\n",
        "\n",
        "embedding_size = 100\n",
        "# Convolution\n",
        "kernel_size = 5\n",
        "filters = 64\n",
        "pool_size = 4\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(vocab_sz+1, embedding_size, mask_zero=True, input_length=maxlen)) No masking allowed for Conv1D\n",
        "model.add(Embedding(vocab_sz+1, embedding_size, input_length=maxlen))\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(MaxPooling1D(pool_size=pool_size))\n",
        "#model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWN2ePtUs6gs",
        "outputId": "104ff980-430d-4f23-c641-56d9037c07eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          1000100   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 96, 64)            32064     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 24, 64)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense (Dense)               (None, 24, 1)             65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,032,229\n",
            "Trainable params: 1,032,229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "gdrive_path = 'gdrive/My Drive/Colab Notebooks/DL NLP Course'\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, Callback\n",
        "model_name = 'basic'\n",
        "filepath = os.path.join(gdrive_path, 'imdb_lstm_' + model_name + '.h5')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLj6H6Bhtc7Y",
        "outputId": "aecaed1c-f942-4957-c274-3453bf016f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks_lst = [checkpoint]\n",
        "# Training\n",
        "batch_size = 256\n",
        "epochs = 20\n",
        "print('Train...')\n",
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,          \n",
        "          validation_data=(x_val, y_val),\n",
        "          #validation_split=0.2,\n",
        "          callbacks=callbacks_lst)\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kayk26htluF",
        "outputId": "5a5a1d63-ebac-40fc-ecf2-0eb5dd83cf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/20\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.5006"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 12s 21ms/step - loss: 0.2502 - accuracy: 0.5006 - val_loss: 0.2500 - val_accuracy: 0.4984\n",
            "Epoch 2/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.2501 - accuracy: 0.5009"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.5019 - val_loss: 0.2502 - val_accuracy: 0.4963\n",
            "Epoch 3/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.2501 - accuracy: 0.4983"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2501 - accuracy: 0.4978 - val_loss: 0.2501 - val_accuracy: 0.4971\n",
            "Epoch 4/20\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.2501 - accuracy: 0.5006"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 10ms/step - loss: 0.2501 - accuracy: 0.5007 - val_loss: 0.2500 - val_accuracy: 0.5012\n",
            "Epoch 5/20\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.4954"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 10ms/step - loss: 0.2501 - accuracy: 0.4954 - val_loss: 0.2500 - val_accuracy: 0.5008\n",
            "Epoch 6/20\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.2501 - accuracy: 0.5007"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2501 - accuracy: 0.5004 - val_loss: 0.2501 - val_accuracy: 0.4986\n",
            "Epoch 7/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.4984"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 10ms/step - loss: 0.2500 - accuracy: 0.4987 - val_loss: 0.2500 - val_accuracy: 0.4972\n",
            "Epoch 8/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.4963"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.4965 - val_loss: 0.2500 - val_accuracy: 0.4978\n",
            "Epoch 9/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.5009"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 10ms/step - loss: 0.2500 - accuracy: 0.5011 - val_loss: 0.2500 - val_accuracy: 0.4979\n",
            "Epoch 10/20\n",
            "52/59 [=========================>....] - ETA: 0s - loss: 0.2500 - accuracy: 0.4986"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.4973 - val_loss: 0.2500 - val_accuracy: 0.4972\n",
            "Epoch 11/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.5022"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 10ms/step - loss: 0.2500 - accuracy: 0.5019 - val_loss: 0.2500 - val_accuracy: 0.4971\n",
            "Epoch 12/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.5013"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.5012 - val_loss: 0.2500 - val_accuracy: 0.4997\n",
            "Epoch 13/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.4986"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.4990 - val_loss: 0.2501 - val_accuracy: 0.4962\n",
            "Epoch 14/20\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.2500 - accuracy: 0.5006"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 10ms/step - loss: 0.2500 - accuracy: 0.5007 - val_loss: 0.2500 - val_accuracy: 0.4965\n",
            "Epoch 15/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.5015"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.5017 - val_loss: 0.2500 - val_accuracy: 0.4963\n",
            "Epoch 16/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.5010"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.5018 - val_loss: 0.2500 - val_accuracy: 0.4966\n",
            "Epoch 17/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.5023"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.5024 - val_loss: 0.2500 - val_accuracy: 0.4964\n",
            "Epoch 18/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.5017"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.5018 - val_loss: 0.2500 - val_accuracy: 0.4963\n",
            "Epoch 19/20\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.2500 - accuracy: 0.5016"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.5024 - val_loss: 0.2500 - val_accuracy: 0.4963\n",
            "Epoch 20/20\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.2500 - accuracy: 0.5024"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.5024 - val_loss: 0.2500 - val_accuracy: 0.4963\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape , y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZSV8w19twYs",
        "outputId": "eac86cbf-4971-42e7-ea5b-835955030d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15000,) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "\n",
        "\n",
        "embedding_size = 100\n",
        "# Convolution\n",
        "kernel_size = 5\n",
        "filters = 64\n",
        "pool_size = 4\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(vocab_sz+1, embedding_size, mask_zero=True, input_length=maxlen)) No masking allowed for Conv1D\n",
        "model.add(Embedding(vocab_sz+1, embedding_size, input_length=maxlen))\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "#model.add(MaxPooling1D(pool_size=pool_size))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2Hwx18uuEO5",
        "outputId": "ff542b9a-9547-4ca7-c573-f40605d68bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          1000100   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 96, 64)            32064     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 64)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,032,229\n",
            "Trainable params: 1,032,229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "from keras.callbacks import EarlyStopping\n",
        "callbacks_lst = [EarlyStopping(monitor='val_accuracy', mode='max')]\n",
        "# Training\n",
        "batch_size = 256\n",
        "epochs = 20\n",
        "print('Train...')\n",
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,          \n",
        "          validation_data=(x_val, y_val),\n",
        "          #validation_split=0.2,\n",
        "          callbacks=callbacks_lst)\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLkO-HNGuyVj",
        "outputId": "d1d642a5-a76d-4cfd-a3fc-2dad8153afa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/20\n",
            "59/59 [==============================] - 2s 18ms/step - loss: 0.6637 - accuracy: 0.6865 - val_loss: 0.5811 - val_accuracy: 0.7959\n",
            "Epoch 2/20\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.4485 - accuracy: 0.8189 - val_loss: 0.3976 - val_accuracy: 0.8225\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-u5E1b0hu4Ws",
        "outputId": "6aeec4f2-106d-4b94-d3e9-47fc67be7049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.clf()   # clear figure\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "3CXcmhpWvN3I",
        "outputId": "9c0fea84-ab38-459f-e53f-903e1b6d195d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xXdb3v8deb4SYXuZvIgDMmippyGzEh85ImphuyNEGOSXUOiZfSs81tZWWa52G7i22P5t60Tc0sNCvDk27vqTs1GQ0wMBV1kFFUQrmJCAOf88daA7/5zZphgFlzfT8fj99j1uW71u/z/f1gfX7ru9b3uxQRmJmZFevS2gGYmVnb5ARhZmaZnCDMzCyTE4SZmWVygjAzs0xOEGZmlskJwppM0r2Szm7usq1JUpWk43PYb0jaP53+d0nfakrZXXifGZLu39U4zRoj94Po2CStL5jtBXwAbEnnvxwRt7V8VG2HpCrgf0bEg8283wBGRsTS5iorqQx4FegWETXNEadZY7q2dgCWr4joUzvd2MFQUlcfdKyt8L/HtsFNTJ2UpGMkVUv6F0lvAjdJGiDp/0laKenddLq0YJs/Sfqf6fRMSf8t6Ydp2VclnbSLZcslPSZpnaQHJV0v6ZcNxN2UGK+U9Od0f/dLGlyw/ixJyyStkvTNRj6fIyS9KamkYNmpkhal0xMkPSlptaQVkq6T1L2Bfd0s6XsF819Lt3lD0heLyp4s6a+S1kpaLunygtWPpX9XS1ov6cjaz7Zg+4mS5ktak/6d2NTPZic/54GSbkrr8K6kuwrWTZW0IK3Dy5Imp8vrNOdJurz2e5ZUlja1fUnSa8DD6fLfpN/DmvTfyCEF2+8h6Ufp97km/Te2h6Q/SrqgqD6LJJ2aVVdrmBNE57Y3MBDYF5hF8u/hpnR+BPA+cF0j2x8BvAAMBv4VuFGSdqHsr4CngUHA5cBZjbxnU2I8E/gCsBfQHbgYQNLBwA3p/vdJ36+UDBHxF+A94Lii/f4qnd4CXJTW50jgE8C5jcRNGsPkNJ4TgJFA8fWP94DPA/2Bk4HZkj6drvt4+rd/RPSJiCeL9j0Q+CNwbVq3HwN/lDSoqA71PpsMO/qcbyVpsjwk3dc1aQwTgF8AX0vr8HGgqqHPI8PRwEHAien8vSSf017As0Bhk+gPgfHARJJ/x5cAW4FbgP9RW0jSaGAYyWdjOyMi/OokL5L/qMen08cAm4CejZQfA7xbMP8nkiYqgJnA0oJ1vYAA9t6ZsiQHnxqgV8H6XwK/bGKdsmK8rGD+XOC/0ulvA3ML1vVOP4PjG9j394Cfp9N9SQ7e+zZQ9kLg9wXzAeyfTt8MfC+d/jlwdUG5AwrLZuz3J8A16XRZWrZrwfqZwH+n02cBTxdt/yQwc0efzc58zsBQkgPxgIxy/1Ebb2P//tL5y2u/54K67ddIDP3TMv1IEtj7wOiMcj2Bd0mu60CSSH7a0v/fOsLLZxCd28qI2Fg7I6mXpP9IT9nXkjRp9C9sZinyZu1ERGxIJ/vsZNl9gHcKlgEsbyjgJsb4ZsH0hoKY9incd0S8B6xq6L1IzhY+I6kH8Bng2YhYlsZxQNrs8mYax/8hOZvYkToxAMuK6neEpEfSpp01wDlN3G/tvpcVLVtG8uu5VkOfTR07+JyHk3xn72ZsOhx4uYnxZtn22UgqkXR12ky1lu1nIoPTV8+s90r/Td8O/A9JXYDpJGc8tpOcIDq34lvY/hk4EDgiIvZke5NGQ81GzWEFMFBSr4Jlwxspvzsxrijcd/qegxoqHBFLSA6wJ1G3eQmSpqq/k/xK3RP4xq7EQHIGVehXwDxgeET0A/69YL87uuXwDZImoUIjgNebEFexxj7n5STfWf+M7ZYDH25gn++RnD3W2jujTGEdzwSmkjTD9SM5y6iN4R/Axkbe6xZgBknT34Yoao6zpnGCsEJ9SU7bV6ft2d/J+w3TX+SVwOWSuks6EvinnGK8EzhF0sfSC8pXsOP/A78CvkpygPxNURxrgfWSRgGzmxjDHcBMSQenCao4/r4kv843pu35ZxasW0nStLNfA/u+BzhA0pmSuko6AzgY+H9NjK04jszPOSJWkFwb+Gl6MbubpNoEciPwBUmfkNRF0rD08wFYAExLy1cApzUhhg9IzvJ6kZyl1cawlaS57seS9knPNo5Mz/ZIE8JW4Ef47GGXOUFYoZ8Ae5D8OnsK+K8Wet8ZJBd6V5G0+99OcmDIsssxRsRi4DySg/4Kknbq6h1s9muSC6cPR8Q/CpZfTHLwXgf8LI25KTHcm9bhYWBp+rfQucAVktaRXDO5o2DbDcBVwJ+V3D310aJ9rwJOIfn1v4rkou0pRXE31Y4+57OAzSRnUW+TXIMhIp4muQh+DbAGeJTtZzXfIvnF/y7wXeqekWX5BckZ3OvAkjSOQhcDzwHzgXeA71P3mPYL4FCSa1q2C9xRztocSbcDf4+I3M9grOOS9HlgVkR8rLVjaa98BmGtTtLhkj6cNklMJml3vmtH25k1JG2+OxeY09qxtGdOENYW7E1yC+Z6knv4Z0fEX1s1Imu3JJ1Icr3mLXbcjGWNcBOTmZll8hmEmZll6jCD9Q0ePDjKyspaOwwzs3blmWee+UdEDMla12ESRFlZGZWVla0dhplZuyKpuPf9Nm5iMjOzTE4QZmaWyQnCzMwyOUGYmVkmJwgzM8vkBGFmZpmcIMzMLFOH6QdhZtZZvPsuVFXBq68mf/v0gVmzmv99nCDMzNqYdevqJoDiv2vW1C3/0Y+2wwSRDt38b0AJ8J8RcXXR+hEkjwbsn5a5NCLukXQCcDXQneSh8l+LiOIHq5iZtUsbNsCyZQ0ngFVFT0rv1QvKy6GsDI46KvlbO19eDv2zHv7aDHJLEOnDza8HTiB5atd8SfPS5/zWugy4IyJukHQwySMTy0ieYvVPEfGGpI8A91H3wetmZm3WBx/Aa681fBbw1lt1y/foAfvumxzsKyrqHvzLy2HwYFDGE89vuw0+85nkvUaMgKuughkzmq8eeZ5BTACWRsQrAJLmkjwIpjBBBLBnOt2P5KHrFD0LYDGwh6QeEdHQYyjNzFpMTQ0sX95wAnjjDSh8kkLXrskBvLwcTjmlbgIoK4O994YuO3nL0G23Jc1KGzYk88uWbW9maq4kkWeCGAYsL5ivBo4oKnM5cL+kC4DewPEZ+/ks8GxWcpA0C5gFMGLEiGYI2cwMtmyBFSsabgJavjwpU6tLFygtTQ74xx9fPwEMGwYlJc0b4ze/uT051NqwIVneHhJEU0wHbo6IH0k6ErhV0kciYiuApENIHkT+yayNI2IO6SMFKyoq/OQjM2uSiKSZp6EEsGwZbN5cd5t99kkO9pMm1b8GMHw4dOvWsnV47bWdW74r8kwQrwPDC+ZL02WFvgRMBoiIJyX1BAYDb0sqBX4PfD4iXs4xTjPrYCKSC721B/3iBFBVBRs31t1myJDkYD9+PHz2s3UTwIgR0LNni1ejUSNGJIksa3lzyTNBzAdGSionSQzTgDOLyrwGfAK4WdJBQE9gpaT+wB9J7mr6c44xmlk7tXp147eCrl9ft/yAAcnB/uCD4eST654FlJVB794tXIHddNVVda9BQHK301VXNd975JYgIqJG0vkkdyCVAD+PiMWSrgAqI2Ie8M/AzyRdRHLBemZERLrd/sC3JX073eUnI+LtvOI1s7Zl/frGE8Dq1XXL9+2bHPD32w8+8Yn6CaBfvxauQM5qrzN885v53cWkiI7RdF9RURF+opxZ+/H++0kTSfHBv3b6H/+oW36PPeq3/RdODxiQfSuoNU7SMxFRkbWutS9Sm1kHtWlT430B3nyzbvnu3bf3BRg3rv6dQHvt5QTQ0pwgzGyX1NRAdXXDCeD11+v2BSgp2d4X4FOfqp8Ahg7d+b4Ali8nCDPLtHVr430BXnutbl8AKekLUFYGxx1Xvzlo2LCkw5i1H/66zDqpCHj77cb7AmzaVHeboUOTA/6RR8L06fX7AnTv3uLVsBw5QZh1UBHwzjsNXwSuqkouFBcaPDg52I8dC6eeWr8vwB57tHg1rBU5QZi1Y2vWNH4r6Lp1dcv3758c7EeNgpNOqn8raJ8+LVwBa9OcIMzasPfe2/5rP6tH8Lvv1i3fp8/2A/6xx9ZPAHkNC20dkxOEWSvauDG7L0Dt35Ur65bv2XP7Qf+jH63fJ2DgQN8Kas3HCcIsR5s3N94XYMWKuuW7ddveF+DTn65/K+iHPuQEYC3HCcJsN2zZktzvn3Xwr+0LsHXr9vIlJcndPmVlMHly/R7BQ4c2/7DQZrvKCcKsEVu3Jj1+G+sLUFOzvbyU3O9fVgbHHFO/L0BpqfsCWPvhf6rWqUUk7fyN9QX4oOhRVXvvnRzwjzgCzjijfl+AHj1avBpmuXCCsA4tIrnTp7FbQYufyjVoUHKwP+wwmDq17lnAvvsmQyqbdQZOENburV3beAJYu7Zu+X79kgP+AQfAiSfWvxW0b98WroBZG+UEYW3ehg11+wIU9wl455265Xv33n7Q//jH6w8L7b4AZk3jBGGt7oMPGu8L8HbRY6J69Nh+sJ8wof6toIMH+1ZQs+bgBGG527w5GRY669d/bV+AwmGhu3XbPiz0lCnZfQE8LLRZ/pwgbLdt2QJvvNHwNYDly+v2BejSZXtfgE9+sv6toPvs474AZm2BE4Tt0Nat8NZbjfcF2Lx5e3kpOciXlcFRR2X3BejWrTVqYmY7wwnCiEie/9vQNYBly5IxgwrttVdysK+ogNNPr9sjeMQI9wUw6whyTRCSJgP/BpQA/xkRVxetHwHcAvRPy1waEfek674OfAnYAnwlIu7LM9aObkd9Ad57r275gQOTg/1HPgL/9E/1bwV1XwCzji+3BCGpBLgeOAGoBuZLmhcRSwqKXQbcERE3SDoYuAcoS6enAYcA+wAPSjogIrZgmdata3xY6DVr6pbv2zc54O+/P5xwQv0EsOeeLVwBM2tz8jyDmAAsjYhXACTNBaYChQkigNpDUT/gjXR6KjA3Ij4AXpW0NN3fkznG26a9/352X4Dav6tW1S3fq9f2g/7HPlb/TqABA3wrqJk1Ls8EMQxYXjBfDRxRVOZy4H5JFwC9geMLtn2qaNthxW8gaRYwC2DEiBHNEnRr+eCDxoeFfuutuuW7d99+wB8/vn4CGDLECcDMdk9rX6SeDtwcET+SdCRwq6SPNHXjiJgDzAGoqKiIHRRvVTU19fsCFDYHvfFG3b4AXbsmF3vLyuCUU+oPC7333u4LYGb5yjNBvA4ML5gvTZcV+hIwGSAinpTUExjcxG3blC1bkg5fjfUF2FJwBaVLl+R2z7IyOP747L4AHhbazFpTnoeg+cBISeUkB/dpwJlFZV4DPgHcLOkgoCewEpgH/ErSj0kuUo8Ens4x1h2KSJp5GhoPaNmyun0BYHtfgEmTsvsCdO/e0rUwM2u63BJERNRIOh+4j+QW1p9HxGJJVwCVETEP+GfgZ5IuIrlgPTMiAlgs6Q6SC9o1wHl538EUkVzobexW0OK+AEOGJAf7cePgs5+tPyx0z555Rmxmli9FtOmm+yarqKiIysrKnd5uxYpkuIeqKli/vu66AQPqX/wtvBW0d+/dj9vMrDVJeiYiKrLWdfpW7oEDYb/94Ljj6ieBfv1aOzozs9bT6RNEjx7whz+0dhRmZm2Pb5Q0M7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpYp1wQhabKkFyQtlXRpxvprJC1IXy9KWl2w7l8lLZb0vKRrJSnPWM3MrK7cnkktqQS4HjgBqAbmS5oXEUtqy0TERQXlLwDGptMTgUnAYenq/waOBv6UV7xmZlZXnmcQE4ClEfFKRGwC5gJTGyk/Hfh1Oh1AT6A70APoBryVY6xmZlYkzwQxDFheMF+dLqtH0r5AOfAwQEQ8CTwCrEhf90XE8xnbzZJUKaly5cqVzRy+mVnn1lYuUk8D7oyILQCS9gcOAkpJkspxko4q3igi5kRERURUDBkypEUDNjPr6PJMEK8DwwvmS9NlWaaxvXkJ4FTgqYhYHxHrgXuBI3OJ0szMMuWZIOYDIyWVS+pOkgTmFReSNAoYADxZsPg14GhJXSV1I7lAXa+JyczM8pNbgoiIGuB84D6Sg/sdEbFY0hWSphQUnQbMjYgoWHYn8DLwHLAQWBgRd+cVq5mZ1ae6x+X2q6KiIiorK1s7DDOzdkXSMxFRkbWurVykNjOzNsYJwszMMjlBmJlZJicIMzPL5ARhZmaZnCDMzCyTE4SZmWVygjAzs0xOEGZmlskJwszMMjlBmJlZJicIMzPL5ARhZmaZnCDMzCyTE4SZmWVygjAzs0xOEGZmlskJwszMMjlBmJlZJicIMzPLlGuCkDRZ0guSlkq6NGP9NZIWpK8XJa0uWDdC0v2Snpe0RFJZnrGamVldXfPasaQS4HrgBKAamC9pXkQsqS0TERcVlL8AGFuwi18AV0XEA5L6AFvzitXMzOrL8wxiArA0Il6JiE3AXGBqI+WnA78GkHQw0DUiHgCIiPURsSHHWM3MrEiTEoSk3pK6pNMHSJoiqdsONhsGLC+Yr06XZe1/X6AceDhddACwWtLvJP1V0g/SM5Li7WZJqpRUuXLlyqZUxczMmqipZxCPAT0lDQPuB84Cbm7GOKYBd0bElnS+K3AUcDFwOLAfMLN4o4iYExEVEVExZMiQZgzHzMyamiCUNvF8BvhpRJwOHLKDbV4HhhfMl6bLskwjbV5KVQML0uapGuAuYFwTYzUzs2bQ5AQh6UhgBvDHdFm9Jp8i84GRksoldSdJAvMydjwKGAA8WbRtf0m1pwXHAUuKtzUzs/w0NUFcCHwd+H1ELJa0H/BIYxukv/zPB+4DngfuSLe9QtKUgqLTgLkREQXbbiFpXnpI0nOAgJ81tVJmZrb7VHBcbtoGycXqPhGxNp+Qdk1FRUVUVla2dhhmZu2KpGcioiJrXVPvYvqVpD0l9Qb+BiyR9LXmDNLMzNqWpjYxHZyeMXwauJfkltSzcovKzMxaXVMTRLe038OngXkRsRnYubYpMzNrV5qaIP4DqAJ6A4+lHdva1DUIMzNrXk0aiykirgWuLVi0TNKx+YRkZmZtQVMvUveT9OPaYS0k/YjkbMLMzDqopjYx/RxYB3wufa0FbsorKDMza31NHe77wxHx2YL570pakEdAZmbWNjT1DOJ9SR+rnZE0CXg/n5DMzKwtaOoZxDnALyT1S+ffBc7OJyQzM2sLmnoX00JgtKQ90/m1ki4EFuUZnJmZtZ6deqJcRKwtGIPpf+cQj5mZtRG788hRNVsUZmbW5uxOgvBQG2ZmHVij1yAkrSM7EQjYI5eIzMysTWg0QURE35YKxMzM2pbdaWIyM7MOzAnCzMwyOUGYmVkmJwgzM8uUa4KQNFnSC5KWSro0Y/01khakrxclrS5av6ekaknX5RmnmZnV19SxmHaapBLgeuAEoBqYL2leRCypLRMRFxWUvwAYW7SbK4HH8orRzMwalucZxARgaUS8EhGbgLnA1EbKTwd+XTsjaTzwIeD+HGM0M7MG5JkghgHLC+ar02X1pM+4LgceTue7AD8CLm7sDSTNqn3K3cqVK5slaDMzS7SVi9TTgDsjYks6fy5wT0RUN7ZRRMyJiIqIqBgyZEjuQZqZdSa5XYMAXgeGF8yXpsuyTAPOK5g/EjhK0rlAH6C7pPURUe9Ct5mZ5SPPBDEfGCmpnCQxTAPOLC4kaRQwAHiydllEzChYPxOocHIwM2tZuTUxRUQNcD5wH/A8cEdELJZ0haQpBUWnAXMjwqPDmpm1Ieoox+WKioqorKxs7TDMzNoVSc9EREXWurZykdrMzNoYJwgzM8vkBGFmZpmcIMzMLJMThJmZZXKCMDOzTE4QZmaWyQnCzMwyOUGYmVkmJwgzM8vkBGFmZpmcIMzMLJMThJmZZXKCMDOzTE4QZmaWyQnCzMwyOUGYmVkmJwgzM8vkBGFmZpmcIMzMLFOuCULSZEkvSFoq6dKM9ddIWpC+XpS0Ol0+RtKTkhZLWiTpjDzjNDOz+rrmtWNJJcD1wAlANTBf0ryIWFJbJiIuKih/ATA2nd0AfD4iXpK0D/CMpPsiYnVe8ZqZWV15nkFMAJZGxCsRsQmYC0xtpPx04NcAEfFiRLyUTr8BvA0MyTFWMzMrkmeCGAYsL5ivTpfVI2lfoBx4OGPdBKA78HLGulmSKiVVrly5slmCNjOzRFu5SD0NuDMithQulDQUuBX4QkRsLd4oIuZEREVEVAwZ4hMMM7PmlGeCeB0YXjBfmi7LMo20eamWpD2BPwLfjIinconQzMwalGeCmA+MlFQuqTtJEphXXEjSKGAA8GTBsu7A74FfRMSdOcZoZmYNyC1BREQNcD5wH/A8cEdELJZ0haQpBUWnAXMjIgqWfQ74ODCz4DbYMXnFamZm9anucbn9qqioiMrKytYOw8ysXZH0TERUZK1rKxepzcysjXGCMDOzTE4QZmaWyQnCzMwyOUGYmVkmJwgzM8vkBGFmZpmcIMzMLJMThJmZZXKCMDOzTE4QZmaWyQnCzMwyOUGYmVkmJwgzM8vkBGFmZpm6tnYAZtb+bd68merqajZu3NjaoVgDevbsSWlpKd26dWvyNk4QZrbbqqur6du3L2VlZUhq7XCsSESwatUqqqurKS8vb/J2bmIys922ceNGBg0a5OTQRkli0KBBO32G5wRhZs3CyaFt25XvxwnCzMwy5ZogJE2W9IKkpZIuzVh/jaQF6etFSasL1p0t6aX0dXaecZpZy7rtNigrgy5dkr+33bZ7+1u1ahVjxoxhzJgx7L333gwbNmzb/KZNmxrdtrKykq985Ss7fI+JEyfuXpDtUG4XqSWVANcDJwDVwHxJ8yJiSW2ZiLiooPwFwNh0eiDwHaACCOCZdNt384rXzFrGbbfBrFmwYUMyv2xZMg8wY8au7XPQoEEsWLAAgMsvv5w+ffpw8cUXb1tfU1ND167Zh7uKigoqKip2+B5PPPHErgXXjuV5BjEBWBoRr0TEJmAuMLWR8tOBX6fTJwIPRMQ7aVJ4AJicY6xm1kK++c3tyaHWhg3J8uY0c+ZMzjnnHI444gguueQSnn76aY488kjGjh3LxIkTeeGFFwD405/+xCmnnAIkyeWLX/wixxxzDPvttx/XXnvttv316dNnW/ljjjmG0047jVGjRjFjxgwiAoB77rmHUaNGMX78eL7yla9s22+hqqoqjjrqKMaNG8e4cePqJJ7vf//7HHrooYwePZpLL00aXZYuXcrxxx/P6NGjGTduHC+//HLzflCNyPM212HA8oL5auCIrIKS9gXKgYcb2XZYxnazgFkAI0aM2P2IzSx3r722c8t3R3V1NU888QQlJSWsXbuWxx9/nK5du/Lggw/yjW98g9/+9rf1tvn73//OI488wrp16zjwwAOZPXt2vb4Df/3rX1m8eDH77LMPkyZN4s9//jMVFRV8+ctf5rHHHqO8vJzp06dnxrTXXnvxwAMP0LNnT1566SWmT59OZWUl9957L3/4wx/4y1/+Qq9evXjnnXcAmDFjBpdeeimnnnoqGzduZOvWrc3/QTWgrfSDmAbcGRFbdmajiJgDzAGoqKiIPAIzs+Y1YkTSrJS1vLmdfvrplJSUALBmzRrOPvtsXnrpJSSxefPmzG1OPvlkevToQY8ePdhrr7146623KC0trVNmwoQJ25aNGTOGqqoq+vTpw3777betn8H06dOZM2dOvf1v3ryZ888/nwULFlBSUsKLL74IwIMPPsgXvvAFevXqBcDAgQNZt24dr7/+OqeeeiqQdHZrSXk2Mb0ODC+YL02XZZnG9ualnd3WzNqRq66C9Bi4Ta9eyfLm1rt3723T3/rWtzj22GP529/+xt13391gn4AePXpsmy4pKaGmpmaXyjTkmmuu4UMf+hALFy6ksrJyhxfRW1OeCWI+MFJSuaTuJElgXnEhSaOAAcCTBYvvAz4paYCkAcAn02Vm1s7NmAFz5sC++4KU/J0zZ9cvUDfVmjVrGDYsaam++eabm33/Bx54IK+88gpVVVUA3H777Q3GMXToULp06cKtt97Kli1Jw8kJJ5zATTfdxIb0As0777xD3759KS0t5a677gLggw8+2La+JeSWICKiBjif5MD+PHBHRCyWdIWkKQVFpwFzo/YqT7LtO8CVJElmPnBFuszMOoAZM6CqCrZuTf7mnRwALrnkEr7+9a8zduzYnfrF31R77LEHP/3pT5k8eTLjx4+nb9++9OvXr165c889l1tuuYXRo0fz97//fdtZzuTJk5kyZQoVFRWMGTOGH/7whwDceuutXHvttRx22GFMnDiRN998s9ljb4gKjsvtWkVFRVRWVrZ2GGad0vPPP89BBx3U2mG0uvXr19OnTx8igvPOO4+RI0dy0UUX7XjDFpL1PUl6JiIy7/N1T2ozs2bys5/9jDFjxnDIIYewZs0avvzlL7d2SLulrdzFZGbW7l100UVt6oxhd/kMwszMMjlBmJlZJicIMzPL5ARhZmaZnCDMrN079thjue++un1pf/KTnzB79uwGtznmmGOovTX+U5/6FKtXr65X5vLLL9/WH6Ehd911F0uWbBukmm9/+9s8+OCDOxN+m+UEYWbt3vTp05k7d26dZXPnzm1wwLxi99xzD/3799+l9y5OEFdccQXHH3/8Lu2rrfFtrmbWrC68ENJHMzSbMWPgJz9peP1pp53GZZddxqZNm+jevTtVVVW88cYbHHXUUcyePZv58+fz/vvvc9ppp/Hd73633vZlZWVUVlYyePBgrrrqKm655Rb22msvhg8fzvjx44Gkj8OcOXPYtGkT+++/P7feeisLFixg3rx5PProo3zve9/jt7/9LVdeeSWnnHIKp512Gg899BAXX3wxNTU1HH744dxwww306NGDsrIyzj77bO6++242b97Mb37zG0aNGlUnpqqqKs466yzee+89AK677rptDy36/ve/zy9/+Zsaj2sAAAoBSURBVEu6dOnCSSedxNVXX83SpUs555xzWLlyJSUlJfzmN7/hwx/+8G597j6DMLN2b+DAgUyYMIF7770XSM4ePve5zyGJq666isrKShYtWsSjjz7KokWLGtzPM888w9y5c1mwYAH33HMP8+fP37buM5/5DPPnz2fhwoUcdNBB3HjjjUycOJEpU6bwgx/8gAULFtQ5IG/cuJGZM2dy++2389xzz1FTU8MNN9ywbf3gwYN59tlnmT17dmYzVu2w4M8++yy33377tqfeFQ4LvnDhQi655BIgGRb8vPPOY+HChTzxxBMMHTp09z5UfAZhZs2ssV/6eaptZpo6dSpz587lxhtvBOCOO+5gzpw51NTUsGLFCpYsWcJhhx2WuY/HH3+cU089dduQ21OmbB827m9/+xuXXXYZq1evZv369Zx44omNxvPCCy9QXl7OAQccAMDZZ5/N9ddfz4UXXggkCQdg/Pjx/O53v6u3fVsYFrzTn0E097Nxzax1TJ06lYceeohnn32WDRs2MH78eF599VV++MMf8tBDD7Fo0SJOPvnkBof53pGZM2dy3XXX8dxzz/Gd73xnl/dTq3bI8IaGC28Lw4J36gRR+2zcZcsgYvuzcZ0kzNqfPn36cOyxx/LFL35x28XptWvX0rt3b/r168dbb721rQmqIR//+Me56667eP/991m3bh133333tnXr1q1j6NChbN68mdsKDhJ9+/Zl3bp19fZ14IEHUlVVxdKlS4FkVNajjz66yfVpC8OCd+oE0VLPxjWzljF9+nQWLly4LUGMHj2asWPHMmrUKM4880wmTZrU6Pbjxo3jjDPOYPTo0Zx00kkcfvjh29ZdeeWVHHHEEUyaNKnOBeVp06bxgx/8gLFjx9Z5XnTPnj256aabOP300zn00EPp0qUL55xzTpPr0haGBe/Uw3136ZKcORSTknHqzaxpPNx3++DhvndCQ8/AzePZuGZm7U2nThAt+WxcM7P2plMniNZ6Nq5ZR9RRmqs7ql35fjp9P4gZM5wQzHZXz549WbVqFYMGDUJSa4djRSKCVatW7XT/iE6fIMxs95WWllJdXc3KlStbOxRrQM+ePSktLd2pbXJNEJImA/8GlAD/GRFXZ5T5HHA5EMDCiDgzXf6vwMkkzWAPAF8Nn8OatUndunWjvLy8tcOwZpZbgpBUAlwPnABUA/MlzYuIJQVlRgJfByZFxLuS9kqXTwQmAbX94f8bOBr4U17xmplZXXlepJ4ALI2IVyJiEzAXmFpU5n8B10fEuwAR8Xa6PICeQHegB9ANeCvHWM3MrEieCWIYsLxgvjpdVugA4ABJf5b0VNokRUQ8CTwCrEhf90XE88VvIGmWpEpJlW77NDNrXq19kborMBI4BigFHpN0KDAYOChdBvCApKMi4vHCjSNiDjAHQNJKSct2I5bBwD92Y/v2qLPVubPVF1znzmJ36rxvQyvyTBCvA8ML5kvTZYWqgb9ExGbgVUkvsj1hPBUR6wEk3QscCTxOAyJiyO4EK6myoe7mHVVnq3Nnqy+4zp1FXnXOs4lpPjBSUrmk7sA0YF5RmbtIkgGSBpM0Ob0CvAYcLamrpG4kF6jrNTGZmVl+cksQEVEDnA/cR3JwvyMiFku6QlLtUzjuA1ZJWkJyzeFrEbEKuBN4GXgOWEhy++vd9d7EzMxyk+s1iIi4B7inaNm3C6YD+N/pq7DMFuDLecaWYU4Lv19b0Nnq3NnqC65zZ5FLnTvMcN9mZta8OvVgfWZm1jAnCDMzy9SpEoSkn0t6W9LfGlgvSddKWippkaRxLR1jc2tCnWekdX1O0hOSRrd0jM1tR3UuKHe4pBpJp7VUbHloSn0lHSNpgaTFkh5tyfjy0IR/1/0k3S1pYVrnL7R0jM1N0nBJj0haktbpqxllmvUY1qkSBHAzMLmR9SeR9MMYCcwCbmiBmPJ2M43X+VXg6Ig4FLiSjnGB72Yar3PtWGHfB+5viYBydjON1FdSf+CnwJSIOAQ4vYXiytPNNP4dnwcsiYjRJLfS/yi93b49qwH+OSIOBj4KnCfp4KIyzXoM61QJIiIeA95ppMhU4BeReAroL2loy0SXjx3VOSKeqB0LC3iK7b3X260mfM8AFwC/Bd7eQbk2rwn1PRP4XUS8lpbvDHUOoK+Sh1P0ScvWtERseYmIFRHxbDq9jqT7QPHwRc16DOtUCaIJmjJ+VEf2JeDe1g4ib5KGAafSMc4Qm+IAYICkP0l6RtLnWzugFnAdyXA9b5D0p/pqRGxt3ZCaj6QyYCzwl6JVzXoMa+2xmKyNkHQsSYL4WGvH0gJ+AvxLRGztJE8/6wqMBz4B7AE8KempiHixdcPK1YnAAuA44MMk47k9HhFrWzes3SepD8nZ74V518cJoq6mjB/V4Ug6DPhP4KS0J3tHVwHMTZPDYOBTkmoi4q7WDSs31cCqiHgPeE/SY8BooCMniC8AV6edcZdKehUYBTzdumHtnnTood8Ct0XE7zKKNOsxzE1Mdc0DPp/eCfBRYE1ErGjtoPIkaQTwO+CsDv6LcpuIKI+IsogoIxnW5dwOnBwA/gB8LB3brBdwBB1/bLPXSM6YkPQh4ECScd7arfR6yo3A8xHx4waKNesxrFOdQUj6NckdDYMlVQPfIXkYERHx7yTDgnwKWApsIPkV0q41oc7fBgYBP01/Ude095Ewm1DnDmVH9Y2I5yX9F7AI2Ery+N9GbwFu65rwHV8J3CzpOUAkTYrtfQjwScBZwHOSFqTLvgGMgHyOYR5qw8zMMrmJyczMMjlBmJlZJicIMzPL5ARhZmaZnCDMzCyTE4TZDkjako6EWvu6tBn3XbajUWfNWkun6gdhtovej4gxrR2EWUvzGYTZLpJUJelf02dpPC1p/3R5maSH0/H4H0p7qyPpQ5J+nz6jYKGkiemuSiT9LB3j/35Je6Tlv5KO/b9I0txWqqZ1Yk4QZju2R1ET0xkF69akz9K4jmQQQID/C9wSEYcBtwHXpsuvBR5Nn1EwDlicLh8JXJ8+q2E18Nl0+aXA2HQ/5+RVObOGuCe12Q5IWh8RfTKWVwHHRcQr6SBqb0bEIEn/AIZGxOZ0+YqIGCxpJVAaER8U7KMMeCAiRqbz/wJ0i4jvpcNjrAfuAu6KiPU5V9WsDp9BmO2eaGB6Z3xQML2F7dcGTwauJznbmC/J1wytRTlBmO2eMwr+PplOPwFMS6dnAI+n0w8BsyF55Kmkfg3tVFIXYHhEPAL8C9CP5MloZi3Gv0jMdmyPgtEzAf4rImpvdR0gaRHJWcD0dNkFwE2SvgasZPuIml8F5kj6EsmZwmygoaGYS4BfpklEwLURsbrZamTWBL4GYbaL0msQFR1gGGmzTG5iMjOzTD6DMDOzTD6DMDOzTE4QZmaWyQnCzMwyOUGYmVkmJwgzM8v0/wGRvV/csx48UAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.layers import LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "\n",
        "\n",
        "embedding_size = 128\n",
        "# Convolution\n",
        "kernel_size = 5\n",
        "filters = 64\n",
        "pool_size = 4\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(vocab_sz+1, embedding_size, mask_zero=True, input_length=maxlen)) No masking allowed for Conv1D\n",
        "#model.add(Embedding(vocab_sz+1, embedding_size, mask_zero=True, input_length=maxlen))\n",
        "model.add(Embedding(vocab_sz+1, embedding_size, input_length=maxlen))\n",
        "#model.add(Embedding(20000, embedding_size, input_length=100))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "model.add(LSTM(70))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygE4xb1NvPgv",
        "outputId": "412b2b6a-4570-4f85-c131-adfb0ac1da5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 128)          1280128   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100, 128)          0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 96, 64)            41024     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 24, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 70)                37800     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 71        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,359,023\n",
            "Trainable params: 1,359,023\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "from keras.callbacks import EarlyStopping\n",
        "callbacks_lst = [EarlyStopping(monitor='val_acc', mode='max')]\n",
        "# Training\n",
        "batch_size = 256\n",
        "epochs = 20\n",
        "print('Train...')\n",
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,          \n",
        "          validation_data=(x_val, y_val),\n",
        "          #validation_split=0.2,\n",
        "          callbacks=callbacks_lst)\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6yTwwlgvvxX",
        "outputId": "6d14827f-3dc5-47fd-b11b-42599faf4f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/20\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.6619"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 6s 33ms/step - loss: 0.5831 - accuracy: 0.6619 - val_loss: 0.3822 - val_accuracy: 0.8342\n",
            "Epoch 2/20\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8847"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 15ms/step - loss: 0.2854 - accuracy: 0.8850 - val_loss: 0.3425 - val_accuracy: 0.8573\n",
            "Epoch 3/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.1712 - accuracy: 0.9386"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.1744 - accuracy: 0.9369 - val_loss: 0.3692 - val_accuracy: 0.8471\n",
            "Epoch 4/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.1116 - accuracy: 0.9631"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 13ms/step - loss: 0.1123 - accuracy: 0.9627 - val_loss: 0.4248 - val_accuracy: 0.8449\n",
            "Epoch 5/20\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9788"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0700 - accuracy: 0.9788 - val_loss: 0.5186 - val_accuracy: 0.8402\n",
            "Epoch 6/20\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9882"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0440 - accuracy: 0.9882 - val_loss: 0.5827 - val_accuracy: 0.8377\n",
            "Epoch 7/20\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9935"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0255 - accuracy: 0.9935 - val_loss: 0.6608 - val_accuracy: 0.8323\n",
            "Epoch 8/20\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.0241 - accuracy: 0.9936"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0262 - accuracy: 0.9928 - val_loss: 0.6889 - val_accuracy: 0.8335\n",
            "Epoch 9/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.0181 - accuracy: 0.9959"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.8247 - val_accuracy: 0.8286\n",
            "Epoch 10/20\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9932"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 0.7167 - val_accuracy: 0.8381\n",
            "Epoch 11/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 0.9961"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0144 - accuracy: 0.9961 - val_loss: 0.6940 - val_accuracy: 0.8317\n",
            "Epoch 12/20\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9966"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 13ms/step - loss: 0.0133 - accuracy: 0.9966 - val_loss: 0.7653 - val_accuracy: 0.8337\n",
            "Epoch 13/20\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9985"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 13ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.8382 - val_accuracy: 0.8254\n",
            "Epoch 14/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.0134 - accuracy: 0.9960"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.8422 - val_accuracy: 0.8315\n",
            "Epoch 15/20\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.0071 - accuracy: 0.9983"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 13ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.8529 - val_accuracy: 0.8325\n",
            "Epoch 16/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 13ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.9236 - val_accuracy: 0.8325\n",
            "Epoch 17/20\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9988"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 13ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.7912 - val_accuracy: 0.8320\n",
            "Epoch 18/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.0225 - accuracy: 0.9921"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.7840 - val_accuracy: 0.8257\n",
            "Epoch 19/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.0199 - accuracy: 0.9937"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 13ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.8061 - val_accuracy: 0.8349\n",
            "Epoch 20/20\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.0053 - accuracy: 0.9990"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/59 [==============================] - 1s 12ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 0.8532 - val_accuracy: 0.8371\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnyfFvNGvxqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}